<!doctype html><html lang=en><head><meta charset=utf-8><meta name=generator content="Hugo 0.76.4"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Anthony"><meta property="og:url" content="https://sudrizzz.github.com/posts/hdfs-file-system/"><link rel=canonical href=https://sudrizzz.github.com/posts/hdfs-file-system/><link rel=alternate type=application/atom+xml href=https://sudrizzz.github.comindex.xml title="Anthony's Blog"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/sudrizzz.github.com"},"articleSection":"posts","name":"HDFS 文件管理","headline":"HDFS 文件管理","description":"通过命令行访问 HDFS 命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。\n对文件和目录的操作 通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。\n1hadoop fs -ls \u0026lt;path\u0026gt; # 列出 path 目录下的所有内容（文件和目录） 2hadoop fs -lsr \u0026lt;path\u0026gt; # 递归列出 path 下的所有内容（文件或目录） 3hadoop fs -df \u0026lt;path\u0026gt; # 查看目录的使用情况 4hadoop fs -du \u0026lt;path\u0026gt; # 显示目录中所有文件及目录大小 5hadoop fs -touchz \u0026lt;path\u0026gt; # 创建一个路径为为 path 的 0 字节的 HDFS 空文件 6hadoop fs -mkdir \u0026lt;path\u0026gt; # 查看目录的使用情况 7hadoop fs -rm [-skipTrash] \u0026lt;path\u0026gt; # 将 HDFS 上路径为 \u0026lt;path\u0026gt; 的文件移动到回收站，加上 -skipTrash，则直接删除 8hadoop fs -rmr [-skipTrash] \u0026lt;path\u0026gt; # 将 HDFS 上路径为 \u0026lt;path\u0026gt; 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除 9hadoop fs -moveFromLocal \u0026lt;localsrc\u0026gt;.","inLanguage":"en-US","author":"Anthony","creator":"Anthony","publisher":"Anthony","accountablePerson":"Anthony","copyrightHolder":"Anthony","copyrightYear":"2020","datePublished":"2020-10-12 15:20:11 \u002b0800 \u002b0800","dateModified":"2020-10-12 15:20:11 \u002b0800 \u002b0800","url":"https:\/\/sudrizzz.github.com\/posts\/hdfs-file-system\/","keywords":[]}</script><title>HDFS 文件管理 - Anthony's Blog</title><meta property="og:title" content="HDFS 文件管理 - Anthony's Blog"><meta property="og:type" content="article"><meta property="og:description" content="通过命令行访问 HDFS 命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。
对文件和目录的操作 通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。
1hadoop fs -ls <path> # 列出 path 目录下的所有内容（文件和目录） 2hadoop fs -lsr <path> # 递归列出 path 下的所有内容（文件或目录） 3hadoop fs -df <path> # 查看目录的使用情况 4hadoop fs -du <path> # 显示目录中所有文件及目录大小 5hadoop fs -touchz <path> # 创建一个路径为为 path 的 0 字节的 HDFS 空文件 6hadoop fs -mkdir <path> # 查看目录的使用情况 7hadoop fs -rm [-skipTrash] <path> # 将 HDFS 上路径为 <path> 的文件移动到回收站，加上 -skipTrash，则直接删除 8hadoop fs -rmr [-skipTrash] <path> # 将 HDFS 上路径为 <path> 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除 9hadoop fs -moveFromLocal <localsrc>."><meta name=description content="通过命令行访问 HDFS 命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。
对文件和目录的操作 通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。
1hadoop fs -ls <path> # 列出 path 目录下的所有内容（文件和目录） 2hadoop fs -lsr <path> # 递归列出 path 下的所有内容（文件或目录） 3hadoop fs -df <path> # 查看目录的使用情况 4hadoop fs -du <path> # 显示目录中所有文件及目录大小 5hadoop fs -touchz <path> # 创建一个路径为为 path 的 0 字节的 HDFS 空文件 6hadoop fs -mkdir <path> # 查看目录的使用情况 7hadoop fs -rm [-skipTrash] <path> # 将 HDFS 上路径为 <path> 的文件移动到回收站，加上 -skipTrash，则直接删除 8hadoop fs -rmr [-skipTrash] <path> # 将 HDFS 上路径为 <path> 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除 9hadoop fs -moveFromLocal <localsrc>."><meta property="og:locale" content="zh-cn"><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.css><link rel=stylesheet href=/css/highlight/tomorrow.min.css><link rel=stylesheet href=/css/index.css><link href=/index.xml rel=alternate type=application/rss+xml title="Anthony's Blog"><link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker|Bree+Serif" rel=stylesheet></head><body><article class="post Chinese" id=article><div class=row><div class=col-xs-12><div class=site-header><header><div class="signatures site-title"><a href=/>Anthony</a></div></header><div class="row end-xs"></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>HDFS 文件管理</h1><div class="row post-desc"><div class=col-xs-6><time class=post-date datetime="2020-10-12 15:20:11 +0800">2020.10.12</time></div><div class=col-xs-6><div class=post-author><a target=_blank href=https://github.com/sudrizzz/>@Anthony</a></div></div></div></header><div class="post-content markdown-body"><h1 id=通过命令行访问-hdfs>通过命令行访问 HDFS</h1><p>命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。</p><h2 id=对文件和目录的操作>对文件和目录的操作</h2><p>通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=ln> 1</span>hadoop fs -ls &lt;path&gt;    <span class=c1># 列出 path 目录下的所有内容（文件和目录）</span>
<span class=ln> 2</span>hadoop fs -lsr &lt;path&gt;   <span class=c1># 递归列出 path 下的所有内容（文件或目录）</span>
<span class=ln> 3</span>hadoop fs -df &lt;path&gt;    <span class=c1># 查看目录的使用情况</span>
<span class=ln> 4</span>hadoop fs -du &lt;path&gt;    <span class=c1># 显示目录中所有文件及目录大小</span>
<span class=ln> 5</span>hadoop fs -touchz &lt;path&gt;    <span class=c1># 创建一个路径为为 path 的 0 字节的 HDFS 空文件</span>
<span class=ln> 6</span>hadoop fs -mkdir &lt;path&gt;     <span class=c1># 查看目录的使用情况</span>
<span class=ln> 7</span>hadoop fs -rm <span class=o>[</span>-skipTrash<span class=o>]</span> &lt;path&gt;   <span class=c1># 将 HDFS 上路径为 &lt;path&gt; 的文件移动到回收站，加上 -skipTrash，则直接删除</span>
<span class=ln> 8</span>hadoop fs -rmr <span class=o>[</span>-skipTrash<span class=o>]</span> &lt;path&gt;  <span class=c1># 将 HDFS 上路径为 &lt;path&gt; 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除</span>
<span class=ln> 9</span>hadoop fs -moveFromLocal &lt;localsrc&gt;...&lt;dst&gt;     <span class=c1># 将 &lt;localsrc&gt; 本地文件移动到 HDFS 的 &lt;dst&gt; 目录下路径下</span>
<span class=ln>10</span>hadoop fs -moveToLocal <span class=o>[</span>-crc<span class=o>]</span> &lt;src&gt; &lt;localdst&gt;  <span class=c1># 将 HDFS 上路径为 &lt;src&gt; 的文件移动到本地 &lt;localdst&gt; 路径下</span>
<span class=ln>11</span>hadoop fs -put &lt;localsrc&gt;...&lt;dst&gt;   <span class=c1># 从本地文件系统中复制单个或者多个源路径到目标文件系统</span>
<span class=ln>12</span>hadoop fs -cat &lt;src&gt;    <span class=c1># 浏览 HDFS 路径为 &lt;src&gt; 的文件的内容</span>
</code></pre></div><h2 id=修改权限或用户组>修改权限或用户组</h2><p>HDFS 提供了一些命令可以用来修改文件的权限、所属用户以及所属组别，具体格式如下:</p><ol><li><p><code>hadoop fs -chmod [-R] &lt;MODE [,MODE]...|OCTALMODE> PATH...</code><br>改变 HDFS 上路径为 PATH 的文件的权限，R 选项表示递归执行该操作。<br>例如: <code>hadoop fs -chmod -R +r /user/test</code>，表示将 /user/test 目录下的所有文件赋予读的权限</p></li><li><p><code>hadoop fs -chown [-R][OWNER][:[GROUP]]PATH...</code><br>改变 HDFS 上路径为 PATH 的文件的所属用户，-R 选项表示递归执行该操作。<br>例如: <code>hadoop fs -chown -R hadoop:hadoop /user/test</code>，表示将 /user/test 目录下所有文件的所属用户和所属组别改为 hadoop</p></li><li><p><code>hadoop fs -chgrp ［-R] GROUP PATH...</code><br>改变 HDFS 上路径为 PATH 的文件的所属组别，-R 选项表示递归执行该操作。<br>例如: <code>hadoop fs -chown -R hadoop /user/test</code> 表示将 /user/test 目录下所有文件的所属组别改为 hadoop</p></li></ol><h2 id=其他命令>其他命令</h2><p>HDFS 除了提供上述两类操作之外，还提供许多实用性较强的操作，如显示指定路径上的内容，上传本地文件到 HDFS 指定文件夹，以及从 HDFS 上下载文件到本地等命令。</p><ol><li><p><code>hadoop fs -tail [-f] &lt;file></code><br>显示 HDFS 上路径为 &lt;file> 的文件的最后 1KB 的字节，-f 选项会使显示的内容随着文件内容更新而更新。<br>例如: <code>hadoop fs -tail -f /user/test.txt</code></p></li><li><p><code>hadoop fs -stat [format] &lt;path></code><br>显示 HDFS 上路径为 &lt;path> 的文件或目录的统计信息。格式为：%b 文件大小，%n 文件名，%r 复制因子，%y、%Y 修改日期。<br>例如：<code>hadoop fs -stat %b %n %o %r /user/test</code></p></li><li><p><code>hadoop fs -put &lt;localsrc>...&lt;dt></code><br>将 &lt;localsrc> 本地文件上传到 HDFS 的 &lt;dst> 目录下。<br>例如: <code>hadoop fs -put /home/hadoop/test.txt /user/hadoop</code></p></li><li><p><code>hadoop fs -count [-q] &lt;path></code><br>显示 &lt;path> 下的目录数及文件数，输出格式为”目录数 文件数 大小 文件名“，加上 -q 可以查看文件索引的情况。<br>例如: <code>hadoop fs -count /</code></p></li><li><p><code>hadoop fs -get [-ignoreCrc] [-crc] &lt;src> &lt;localdst></code><br>将 HDFS 上 &lt;src> 的文件下载到本地的 &lt;localdst> 目录，可用 -ignorecrc 选项复制 CRC 校验失败的文件，使用 -crc 选项复制文件以及 CRC 信息。<br>例如: <code>hadoop fs -get /user/hadoop/a.txt /home/hadoop</code></p></li><li><p><code>hadoop fs -getmerge &lt;src> &lt;localdst> [addnl]</code><br>将 HDFS 上 &lt;src> 目录下的所有文件按文件名排序并合并成一个文件输出到本地的 &lt;localdst> 目录，addnl 是可选的，用于指定在每个文件结尾添加一个换行符。<br>例如: <code>hadoop fs -getmerge /user/test /home/hadoop/o</code></p></li><li><p><code>hadoop fs -test -[ezd] &lt;path></code><br>检查 HDFS 上路径为 &lt;path> 的文件。-e 检查文件是否存在，如果存在则返回 0。-z 检查文件是否为 0 字节，如果是则返回 0。-d 检查路径是否是目录，如果是则返回 1，否则返回 0。<br>例如：<code>hadoop fs -test -e /user/test.txt</code></p></li></ol><h1 id=通过-java-api-访问-hdfs>通过 Java API 访问 HDFS</h1><h2 id=使用-hadoop-url-读取数据>使用 Hadoop URL 读取数据</h2><h2 id=通过-filesystem-api-读取数据>通过 FileSystem API 读取数据</h2><h3 id=读取文件>读取文件</h3><h3 id=写入文件>写入文件</h3><h3 id=创建目录>创建目录</h3><h3 id=删除文件或目录>删除文件或目录</h3><h3 id=列出文件或目录>列出文件或目录</h3><h1 id=小结>小结</h1><h2 id=hdfs-组成部分>HDFS 组成部分</h2><ul><li>HDFS 是一个分布式文件存储系统</li><li>Client 提交读写请求（拆分 blocksize）</li><li>NameNode 全局把控（存储数据位置）</li><li>DataNode 存储数据（将数据存储进去，且以 Pipeline 的方式把数据写完）</li></ul><h2 id=hdfs-数据交互>HDFS 数据交互</h2><h3 id=写入数据>写入数据</h3><ol><li>使用 HDFS 提供的客户端 Client，向远程的 NameNode 发起 RPC 请求</li><li>NameNode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常</li><li>当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列 data queue（数据队列） 的形式管理这些 packets，并向 NameNode 申请 blocks，获取用来存储 replicas 的合适的 DataNode 列表，列表的大小根据 NameNode 中 replication（副本份数）的设定而定</li><li>开始以 pipeline（管道）的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的方式写入第一个 DataNode，该 DataNode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 DataNode，直到最后一个 DataNode，这种写数据的方式呈流水线的形式</li><li>最后一个 DataNode 成功存储之后会返回一个 ack packet（确认队列），在 pipeline 里传递至客户端，在客户端的开发库内部维护着 &ldquo;ack queue&rdquo;，成功收到 DataNode 返回的 ack packet 后会从 &ldquo;data queue&rdquo; 移除相应的 packet</li><li>如果传输过程中，有某个 DataNode 出现了故障，那么当前的 pipeline 会被关闭，出现故障的 DataNode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 DataNode 中继续以 pipeline 的形式传输，同时 NameNode 会分配一个新的 DataNode，保持 replicas 设定的数量。</li><li>客户端完成数据的写入后，会对数据流调用 close() 方法，关闭数据流</li><li>只要写入了 dfs.replication.min（最小写入成功的副本数）的复本数（默认为 1），写操作就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数（dfs.replication 的默认值为 3），因为 NameNode 已经知道文件由哪些块组成，所以它在返回成功前只需要等待数据块进行最小量的复制</li></ol><h3 id=读取数据>读取数据</h3><ol><li>客户端调用 FileSystem 实例的 open 方法，获得这个文件对应的输入流 InputStream</li><li>通过 RPC 远程调用 NameNode，获得 NameNode 中此文件对应的数据块保存位置，包括这个文件的副本的保存位置（主要是各 DataNode 的地址）</li><li>获得输入流之后，客户端调用 read 方法读取数据。选择最近的 DataNode 建立连接并读取数据</li><li>如果客户端和其中一个 DataNode 位于同一机器（比如 MapReduce 过程中的 mapper 和 reducer)，那么就会直接从本地读取数据</li><li>到达数据块末端，关闭与这个 DataNode 的连接，然后重新查找下一个数据块</li><li>不断执行第 2~5 步直到数据全部读完</li><li>客户端调用 close，关闭输入流 DFS InputStream</li></ol><h1 id=参考文章>参考文章</h1><p><a href=https://www.cnblogs.com/qingyunzong/p/8548806.html>https://www.cnblogs.com/qingyunzong/p/8548806.html</a></p></div><div class="row middle-xs"><div class=col-xs-12><div class=post-category><a href=/categories/hadoop>hadoop</a></div></div></div><div class=row><div class=col-xs-12></div></div><div style=height:50px></div><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"],],},};</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><div class=site-footer><div class=site-footer-item><a href=/ target=_self>Home</a></div><div class=site-footer-item><a href=mailto:sudrizzz@google.com target=_blank>Mail</a></div><div class=site-footer-item><a href="https://music.163.com/song?id=356827" target=_blank>~</a></div></div></div></div></article><script src=/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>