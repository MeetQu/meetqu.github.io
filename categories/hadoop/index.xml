<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hadoop on Anthony's Blog</title><link>https://sudrizzz.github.com/categories/hadoop/</link><description>Recent content in hadoop on Anthony's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 16 Oct 2020 20:00:00 +0800</lastBuildDate><atom:link href="https://sudrizzz.github.com/categories/hadoop/index.xml" rel="self" type="application/rss+xml"/><item><title>MapReduce 分布式编程</title><link>https://sudrizzz.github.com/posts/mapreduce-distributed-programming/</link><pubDate>Fri, 16 Oct 2020 20:00:00 +0800</pubDate><guid>https://sudrizzz.github.com/posts/mapreduce-distributed-programming/</guid><description>词频统计程序示例 假设将一个英文文本大文件作为输入，统计文件中单词出现的频数。最基本的操作是把输入文件的每一行传递给 map 函数完成对单词的拆分并输出中间结果，中间结果为 &amp;lt;word, 1&amp;gt; 的形式， 表示程序对一个单词，都对应一个计数 1。使用 reduce 函数收集 map 函数的结果作为输入值，并生成最终 &amp;lt;word, count&amp;gt; 形式的结果，完成对每个单词的词频统计。它们对应 MapReduce 处理数据流程如上图所示。
MapReduce 程序的运行过程 如图所示，MapReduce 运行阶段数据传递经过输入文件、Map 阶段、中间文件、 Reduce 阶段、输出文件五个阶段，用户程序只与 Map 阶段和 Reduce 阶段的 Worker 直接相关，其他事情由 Hadoop 平台根据设置自行完成。
从用户程序 User Program 开始，用户程序 User Program 链接了 MapReduce 库，实现了最基本的 map 函数和 reduce 函数。
MapReduce 库先把 User Program 的输入文件划分为 M 份，如上图左方所示，将数据分成了分片 0~4，每一份通常为 16MB~64MB；然后使用 fork 将用户进程复制到集群内其他机器上。 User Program 的副本中有一个 Master 副本和多个 Worker 副本。Master 是负责调度的，为空闲 Worker 分配 Map 作业或者 Reduce 作业。 被分配了 Map 作业的 Worker，开始读取对应分片的输入数据, Map 作业数量与输入文件划分数 M 相同，并与分片一一对应; Map 作业将输入数据转化为键值对表示形式并传递给 map 函数，map 函数产生的中间键值对被缓存在内存中。 缓存的中间键值对会被定期写入本地磁盘，而且被分为 R 个区（R 的大小是由用户定义的），每个区会对应一个 Reduce 作业；这些中间键值对的位置会被通报给 Master, Master 负责将信息转发给 Reduce Worker。 Master 通知分配了 Reduce 作业的 Worker 负责数据分区，Reduce Worker 读取键值对数据并依据键排序，使相同键的键值对聚集在一起。同一个分区可能存在多个键的键值对，而 reduce 函数的一次调用的键值是唯一的， 所以必须进行排序处理。 Reduce Worker 遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给 reduce 函数，reduce 函数产生的输出会写回到数据分区的输出文件中。 当所有的 Map 和 Reduce 作业都完成了，Master 唤醒 User Program，MapReduce 函数调用返回 User Program。 执行完毕后，MapReduce 的输出放在 R 个分区的输出文件中，即每个 Reduce 作业分别对应一个输出文件。用户可将这 R 个文件作为输入交给另一个 MapReduce 程序处理，而不需要主动合并这 R 个文件。在 MapReduce 计算过程中，输入数据来自分布式文件系统，中间数据放在本地文件系统，最终输出数据写入分布式文件系统。</description></item><item><title>HDFS 文件管理</title><link>https://sudrizzz.github.com/posts/hdfs-file-system/</link><pubDate>Mon, 12 Oct 2020 15:20:11 +0800</pubDate><guid>https://sudrizzz.github.com/posts/hdfs-file-system/</guid><description>本文所有代码均可在 https://github.com/sudrizzz/HDFSOperations 查看。
通过命令行访问 HDFS 命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。
对文件和目录的操作 通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。
1hadoop fs -ls &amp;lt;path&amp;gt; # 列出 path 目录下的所有内容（文件和目录） 2hadoop fs -lsr &amp;lt;path&amp;gt; # 递归列出 path 下的所有内容（文件或目录） 3hadoop fs -df &amp;lt;path&amp;gt; # 查看目录的使用情况 4hadoop fs -du &amp;lt;path&amp;gt; # 显示目录中所有文件及目录大小 5hadoop fs -touchz &amp;lt;path&amp;gt; # 创建一个路径为为 path 的 0 字节的 HDFS 空文件 6hadoop fs -mkdir &amp;lt;path&amp;gt; # 查看目录的使用情况 7hadoop fs -rm [-skipTrash] &amp;lt;path&amp;gt; # 将 HDFS 上路径为 &amp;lt;path&amp;gt; 的文件移动到回收站，加上 -skipTrash，则直接删除 8hadoop fs -rmr [-skipTrash] &amp;lt;path&amp;gt; # 将 HDFS 上路径为 &amp;lt;path&amp;gt; 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除 9hadoop fs -moveFromLocal &amp;lt;localsrc&amp;gt;.</description></item><item><title>初识 Hadoop</title><link>https://sudrizzz.github.com/posts/getting-to-know-hadoop/</link><pubDate>Thu, 24 Sep 2020 09:20:11 +0800</pubDate><guid>https://sudrizzz.github.com/posts/getting-to-know-hadoop/</guid><description>前言 本系列文章是基于《大数据技术基础》与 10 小时入门大数据 课程，如果有兴趣可以先阅读该书并观看视频教程。本系列文章中所用到的软件版本及其下载地址如下：
名称 版本 下载地址 CentOS 8.2.2004 https://mirrors.tuna.tsinghua.edu.cn/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso JDK 14.0.2 https://www.oracle.com/java/technologies/javase/jdk14-archive-downloads.html Hadoop 2.10.1 https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz 环境准备 配置网络 此篇文章所使用的 CentOS 环境均是使用 VMware 15 虚拟的，具体安装教程请查看 使用 VMware 15 安装虚拟机和使用 CentOS 8，此处不再赘述。安装好一个节点之后，我们可以采用“虚拟机克隆”的方式，直接完成另外两个节点系统的安装。
虚拟机的网络配置采用 DHCP 自动分配模式，每台机器的 IP 地址可以通过命令 ip address 或 ifconfig 查看，其中 ifconfig 输出如下，第一组配置中 ens33 即为本机网络配置，inet 项对应的即为本机 ip（192.168.61.128）。
1ens33: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt; mtu 1500 2 inet 192.168.61.128 netmask 255.255.255.0 broadcast 192.</description></item></channel></rss>